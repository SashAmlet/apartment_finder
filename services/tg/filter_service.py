import re
import os
import json
import asyncio
from typing import List, Tuple

from google import genai

from models import Container, TelegramChannel, TelegramMessage
from utils import get_prompt_by_id

from services.base import Service
from services.tg.classifier.base import Classifier
from services.tg.classifier.random_forest import RandomForestMessageClassifier


class TgFilterService(Service):
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ Telegram-—Å–æ–æ–±—â–µ–Ω–∏–π –æ–± –∞—Ä–µ–Ω–¥–µ.
    –î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞:
      1. –ñ–µ—Å—Ç–∫–∏–π (strict) –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª.
      2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π AI (–Ω–∞–ø—Ä–∏–º–µ—Ä, Google Gemini).
    """

    def __init__(self, api_key: str, ai_model: str, ml_model: Classifier, confidence_threshold: float = .8):
        super().__init__()
        
        self.ai_model = ai_model
        self.client = genai.Client(api_key=api_key)
        self.ml_model = ml_model
        self.confidence_threshold = confidence_threshold

    @classmethod
    async def create(cls, api_key: str, ml_model_path: str, ml_model = None, ml_model_name: str="RandomForest", ai_model: str = "gemini-2.5-flash-lite", confidence_threshold: float = .8) -> "TgFilterService":
        
        if ml_model is None:
            if ml_model_name == "RandomForest":
                ml_model = RandomForestMessageClassifier()

            ml_model_path = os.path.join(os.path.dirname(__file__), "..", "..", ml_model_path)
            await ml_model.load(ml_model_path)
        else:
            assert isinstance(ml_model, Classifier), "ml_model must be an instance of Classifier"

        return cls(api_key, ai_model, ml_model, confidence_threshold)


    async def run(self, container: Container) -> Container:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥:
        1. –î–µ–ª–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–∞ strict_accept / reject / ambiguous.
        2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç ambiguous —á–µ—Ä–µ–∑ Gemini.
        3. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ strict_accept.
        """
        all_channels = []
        channels: List[TelegramChannel] = container.channels

        for channel in channels:
            if not channel.messages:
                all_channels.append(channel)
                continue
            
            strict_accept: List[TelegramMessage] = []
            ambiguous: List[TelegramMessage] = []

            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –∫–∞–Ω–∞–ª–∞
            strict_accept, rej, ambiguous = await self.classify_messages(channel.messages)

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —á–µ—Ä–µ–∑ Gemini
            gemini_accept: List[TelegramMessage] = []
            if ambiguous:
                gemini_accept, _ = await self.ai_analyzer(ambiguous)

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥
            channel.messages = strict_accept + gemini_accept
            all_channels.append(channel)

        return Container(channels=all_channels)



    async def classify_messages(
        self, messages: List[TelegramMessage]
    ) -> Tuple[List[TelegramMessage], List[TelegramMessage], List[TelegramMessage]]:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂:
          (strict_accept, reject, ambiguous)
        """
        accept, reject, ambiguous = [], [], []

        pred_result = await self.ml_model.predict_with_confidence(messages)

        for msg, res in zip(messages, pred_result):
            if res["confidence"] >= self.confidence_threshold:
                if res["class"] == 1:
                    accept.append(msg)
                else:
                    reject.append(msg)
            else:
                ambiguous.append(msg)

        return accept, reject, ambiguous
    
    def _clean_ai_response_text(self, raw_text: str) -> str:
        """
        –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏ –æ—Ç –º—É—Å–æ—Ä–∞, –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–π –∏
        –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∫ JSON-–¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏.
        """
        if not raw_text:
            return ""

        cleaned = raw_text.strip()

        # 1Ô∏è‚É£ –£–¥–∞–ª—è–µ–º Markdown/LLM-–º–∞—Ä–∫–µ—Ä—ã –≤—Ä–æ–¥–µ ```json``` –∏–ª–∏ ```
        cleaned = re.sub(r"(?s)```json|```", "", cleaned).strip()

        # 2Ô∏è‚É£ –ü–æ–ø—Ä–∞–≤–ª—è–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        # (–∑–∞–º–µ–Ω—è–µ–º –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –æ–±—Ä–∞—Ç–Ω—ã–µ —Å–ª—ç—à–∏ –Ω–∞ –¥–≤–æ–π–Ω—ã–µ, –∫—Ä–æ–º–µ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö)
        cleaned = re.sub(r'(?<!\\)\\(?!["\\/bfnrtu])', r'\\\\', cleaned)
        cleaned = cleaned.replace('\\\\\"', '\\"')

        # 3Ô∏è‚É£ –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –∫–∞–≤—ã—á–∫–∏ –≤–Ω—É—Ç—Ä–∏ "text"
        cleaned = re.sub(
            r'("text":\s*")((?:[^"\\]|\\.)*)"',
            lambda m: '"text": "{}"'.format(m.group(2).replace('"', r'\"')),
            cleaned
        )

        return cleaned
    
    async def ai_analyzer(self, messages: List[TelegramMessage]) -> Tuple[List[TelegramMessage], List[TelegramMessage]]:
        """
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Google Gemini –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ 5.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ —Å–¥–∞—á–µ –∂–∏–ª—å—è.
        """

        if not messages:
            return []

        prompts_path = os.path.join(
            os.path.dirname(__file__), "..", "..", "promts", "tg_filter_service.json"
        )
        system, user_template = get_prompt_by_id(prompts_path, "1")

        accepted = []
        rejected = []
        batch_size = 10

        # —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ batch_size —Å–æ–æ–±—â–µ–Ω–∏–π
        for i in range(0, len(messages), batch_size):
            batch = messages[i:i+batch_size]

            # —Ñ–æ—Ä–º–∏—Ä—É–µ–º user-–ø—Ä–æ–º—Ç
            batch_texts = []
            for idx, msg in enumerate(batch, start=1):
                batch_texts.append(f'{idx}) id: {msg.sender}, text: "{msg.text}"')
            user = user_template.format(input_data=batch_texts)

            try:
                response = await asyncio.to_thread(
                    self.client.models.generate_content,
                    model=self.ai_model,
                    contents=[system, user],
                )

                raw_text = response.text.strip()

                # üßπ –£–¥–∞–ª—è–µ–º LLM-–º–∞—Ä–∫–µ—Ä—ã –∏ –º—É—Å–æ—Ä
                cleaned = self._clean_ai_response_text(raw_text) 

                # üß© –ò—â–µ–º JSON-–º–∞—Å—Å–∏–≤ –≤ —Ç–µ–∫—Å—Ç–µ
                match = re.search(r"\[.*\]", cleaned, re.DOTALL)
                if not match:
                    print(f"‚ö†Ô∏è JSON –º–∞—Å—Å–∏–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ –±–∞—Ç—á–∞ {i//batch_size + 1}")
                    continue

                json_text = match.group(0)

                try:
                    result_json = json.loads(json_text)
                except json.JSONDecodeError as e:
                    print(f"‚ö†Ô∏è JSONDecodeError –≤ –±–∞—Ç—á–µ {i//batch_size + 1}: {e}")
                    print(f"‚öôÔ∏è –ü—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ eval-–∑–∞—â–∏—â—ë–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥...")
                    # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ fallback-—Å–ø–æ—Å–æ–±–æ–º
                    safe_text = json_text.replace("'", '"')
                    result_json = json.loads(safe_text)

                for obj, msg in zip(result_json, batch):
                    if obj.get("offer"):
                        accepted.append(msg)
                    else:
                        rejected.append(msg)

            except Exception as e:
                print(f"ai_analyzer ERROR::\n {i//10 + 1}: {e}\n{response.text if 'response' in locals() else ''}")

        return accepted, rejected
    